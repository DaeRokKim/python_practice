{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "이미지 크롤링.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1v8IPPKahh7"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# 접근할 페이지 번호\n",
        "pageNum = 2\n",
        "\n",
        "# 저장할 이미지 경로 및 이름\n",
        "imageNum = 0\n",
        "imageDir = \"/content/drive/MyDrive/TON2/sunset/k_\"\n",
        "\n",
        "while pageNum < 320: # 필요한 페이지에 맞추기\n",
        "  url1 = \"https://wallpaperscraft.com/tag/sunset/page\"\n",
        "  # url2 = \"&query=sunset&size=\"\n",
        "  url = url1 + str(pageNum) #+ url2\n",
        "\n",
        "  fp = urllib.request.urlopen(url)\n",
        "  source = fp.read();\n",
        "  fp.close()\n",
        "\n",
        "  soup = BeautifulSoup(source, \"html.parser\")\n",
        "  soup = soup.findAll(\"span\", class_ = \"wallpapers__canvas\")\n",
        "\n",
        "  # 이미지 경로 받아 로컬에 저장\n",
        "  for i in soup:\n",
        "    imageNum += 1\n",
        "    imgURL = i.find(\"img\")[\"src\"]\n",
        "    urllib.request.urlretrieve(imgURL, imageDir + str(imageNum).zfill(4) + \".jpg\")\n",
        "    # urllib.request.urlretrieve(\"https:\" + imgURL, imageDir + str(imageNum).zfill(4) + \".jpg\") # https: 없을때\n",
        "    print(imgURL)\n",
        "    print(imageNum)\n",
        "\n",
        "  pageNum += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# 접근할 페이지 번호\n",
        "pageNum = 2\n",
        "\n",
        "# 저장할 이미지 경로 및 이름\n",
        "imageNum = \n",
        "imageDir = \"/content/drive/MyDrive/TON2/sunset/k_\"\n",
        "\n",
        "while pageNum < 320: # 필요한 페이지에 맞추기\n",
        "  url1 = \"http://wallpaperswide.com/search/page/2?q=sunset\"\n",
        "  url2 = \"?q=sunset\"\n",
        "  url = url1 + str(pageNum) + url2\n",
        "\n",
        "  fp = urllib.request.urlopen(url)\n",
        "  source = fp.read();\n",
        "  fp.close()\n",
        "\n",
        "  soup = BeautifulSoup(source, \"html.parser\")\n",
        "  soup = soup.findAll(\"div\", class_ = \"thumb\")\n",
        "\n",
        "  # 이미지 경로 받아 로컬에 저장\n",
        "  for i in soup:\n",
        "    imageNum += 1\n",
        "    imgURL = i.find(\"img\")[\"src\"]\n",
        "    urllib.request.urlretrieve(imgURL, imageDir + str(imageNum).zfill(4) + \".jpg\")\n",
        "    # urllib.request.urlretrieve(\"https:\" + imgURL, imageDir + str(imageNum).zfill(4) + \".jpg\") # https: 없을때\n",
        "    print(imgURL)\n",
        "    print(imageNum)\n",
        "\n",
        "  pageNum += 1"
      ],
      "metadata": {
        "id": "UcjYokfobHEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import concurrent.futures\n",
        "from multiprocessing import Pool \n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "import requests\n",
        "import time\n",
        "import os \n",
        "\n",
        "image_num = 0\n",
        "img_folder = './space' #생성할 이미지 폴더\n",
        "if not os.path.isdir(img_folder) : \n",
        "    os.mkdir(img_folder)\n",
        "\n",
        "def get_sublist_href():\n",
        "    links = []\n",
        "    pageNum = 2\n",
        "    while pageNum<126:\n",
        "        url = \"https://wallpaperscraft.com/catalog/space/page\" #순회할 페이지 : 페이지url+숫자 형식\n",
        "        url = url+str(pageNum)\n",
        "        response = requests.get(url)\n",
        "        html = response.text\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        ul = soup.select_one('ul.wallpapers__list') #이미지를 담고있는 리스트를 가져옴\n",
        "\n",
        "        data = ul.select('li > a') #여기서 링크만 선택\n",
        "        for d in data:\n",
        "            links.append(d.attrs['href']) #links에 이미지 url주소를 다 담음\n",
        "        \n",
        "        pageNum+=1\n",
        "    return links\n",
        "\n",
        "def do_html_crawl(link):\n",
        "    #global image_num\n",
        "    abs_link = 'https://wallpaperscraft.com'+link #홈페이지 기본주소 + 이미지 url 형태의 절대 경로를 생성\n",
        "    req = requests.get(abs_link)\n",
        "    html = req.text\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    try:\n",
        "        img_url = soup.select_one('img.wallpaper__image')['src'] #절대경로에서 확대된 이미지 소스를 들고옴\n",
        "    except TypeError:\n",
        "        import pdb;pdb.set_trace()\n",
        "    urllib.request.urlretrieve(img_url, f'{img_folder}/{link[-6:]}.jpg') #확대된 이미지 저장(멀티프로세싱이라 num=0 -> num+=1식으로 저장 하면 안됨)\n",
        "    #image_num+=1\n",
        "\n",
        "def do_thread_crawl(urls):\n",
        "    thread_list = []\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        for url in urls:\n",
        "            thread_list.append(executor.submit(do_html_crawl, url))\n",
        "        for execution in concurrent.futures.as_completed(thread_list):\n",
        "            execution.result()\n",
        "    \n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    start_time = time.time()\n",
        "    do_thread_crawl(get_sublist_href())\n",
        "    print(f\"--- {(time.time() - start_time):.2f}seconds ---\")"
      ],
      "metadata": {
        "id": "PV7LIhCaT1oI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}