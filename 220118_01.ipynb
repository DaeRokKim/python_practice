{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f06977dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()\n",
    "    \n",
    "print(raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd16fc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ead78cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def change_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 소문자로 변경, 양쪽 공백 삭제\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 특수문자 양쪽 공백 삽입\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개의 공백 하나로\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿가 아닌 문자를 하나의 공백으로 변경\n",
    "    sentence = sentence.strip() # 양쪽 공백 삭제\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 문장 시작에 <start>, 끝에 <end>를 추가한다.\n",
    "    return sentence\n",
    "    \n",
    "print(change_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1494bd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0:\n",
    "        continue\n",
    "    if sentence[-1] == \":\":\n",
    "        continue\n",
    "    \n",
    "    changed_sentence = change_sentence(sentence)\n",
    "    corpus.append(changed_sentence)\n",
    "    \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6871c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f5fea13bf40>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=7000, filters='', oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "    \n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4a72cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98f397e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c7d16c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "source_input = tensor[:, :-1]\n",
    "target_input = tensor[:, 1:]\n",
    "\n",
    "print(source_input[0])\n",
    "print(target_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b07313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(source_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(source_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((source_input, target_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e15a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb844f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[-4.64363227e-04,  1.61830278e-04, -3.14673467e-04, ...,\n",
       "          4.21569595e-04,  2.52897298e-04, -1.32707180e-04],\n",
       "        [-8.26138305e-04, -9.70383044e-05, -3.48264526e-04, ...,\n",
       "          5.59910201e-04,  1.45858095e-04,  1.31319694e-05],\n",
       "        [-1.29641965e-03, -2.81053624e-04, -2.56651314e-04, ...,\n",
       "          7.75502063e-04,  2.01173767e-04,  2.69232027e-04],\n",
       "        ...,\n",
       "        [-1.81228854e-03, -2.30299099e-03,  3.20459087e-03, ...,\n",
       "          9.67934902e-04, -1.30347034e-03,  3.76898068e-04],\n",
       "        [-2.26381375e-03, -2.25120666e-03,  3.66648799e-03, ...,\n",
       "          9.80452402e-04, -1.47522951e-03,  3.46892659e-04],\n",
       "        [-2.68977555e-03, -2.16462673e-03,  4.08664485e-03, ...,\n",
       "          9.81136574e-04, -1.60059496e-03,  3.17072991e-04]],\n",
       "\n",
       "       [[-4.64363227e-04,  1.61830278e-04, -3.14673467e-04, ...,\n",
       "          4.21569595e-04,  2.52897298e-04, -1.32707180e-04],\n",
       "        [-5.26450109e-04, -2.72942358e-04, -5.41981426e-04, ...,\n",
       "          6.95103721e-04,  1.69075996e-04, -2.79359781e-04],\n",
       "        [-3.74524534e-04, -4.81711613e-04, -3.77582386e-04, ...,\n",
       "          1.07765594e-03,  1.29082648e-04, -4.76000860e-04],\n",
       "        ...,\n",
       "        [-1.98143837e-03, -1.83028006e-03,  2.07139296e-03, ...,\n",
       "          1.21847587e-03, -3.86997097e-04,  1.15022631e-05],\n",
       "        [-2.33741291e-03, -1.87267864e-03,  2.73782061e-03, ...,\n",
       "          1.14400161e-03, -6.97027077e-04,  1.70846513e-04],\n",
       "        [-2.69782543e-03, -1.85667130e-03,  3.33565427e-03, ...,\n",
       "          1.07925583e-03, -9.51656315e-04,  2.80323875e-04]],\n",
       "\n",
       "       [[-4.64363227e-04,  1.61830278e-04, -3.14673467e-04, ...,\n",
       "          4.21569595e-04,  2.52897298e-04, -1.32707180e-04],\n",
       "        [-5.77607774e-04,  1.20230565e-04, -7.01644283e-04, ...,\n",
       "          5.99734660e-04,  3.29091330e-04, -2.21178430e-04],\n",
       "        [-5.20964444e-04, -2.91709839e-05, -1.11807243e-03, ...,\n",
       "          5.19406109e-04,  4.20344702e-04, -2.40549212e-04],\n",
       "        ...,\n",
       "        [-1.59003155e-03, -1.91814033e-03,  3.32038198e-03, ...,\n",
       "          5.01552538e-04, -1.28146505e-03, -1.54974827e-04],\n",
       "        [-2.05166033e-03, -1.89554389e-03,  3.74504738e-03, ...,\n",
       "          5.01269475e-04, -1.48366706e-03, -6.86701169e-05],\n",
       "        [-2.49055703e-03, -1.83815428e-03,  4.13783779e-03, ...,\n",
       "          5.08179422e-04, -1.63347379e-03, -7.67034464e-07]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-4.64363227e-04,  1.61830278e-04, -3.14673467e-04, ...,\n",
       "          4.21569595e-04,  2.52897298e-04, -1.32707180e-04],\n",
       "        [-4.33143927e-04, -1.89570768e-04, -5.49139397e-04, ...,\n",
       "          4.64527111e-04,  5.06591517e-04, -1.13380913e-04],\n",
       "        [-6.94534101e-05, -5.13632142e-04, -8.72645236e-04, ...,\n",
       "          1.41247774e-05,  8.87884235e-04, -4.25905746e-05],\n",
       "        ...,\n",
       "        [-3.00589227e-03, -1.88061001e-03,  4.07558680e-03, ...,\n",
       "          1.08638871e-03, -1.01210724e-03,  8.04604613e-04],\n",
       "        [-3.38126044e-03, -1.75665482e-03,  4.52381326e-03, ...,\n",
       "          1.07540668e-03, -1.11604459e-03,  7.11259025e-04],\n",
       "        [-3.70812463e-03, -1.63908280e-03,  4.89744777e-03, ...,\n",
       "          1.05778861e-03, -1.18327641e-03,  6.16470119e-04]],\n",
       "\n",
       "       [[-4.64363227e-04,  1.61830278e-04, -3.14673467e-04, ...,\n",
       "          4.21569595e-04,  2.52897298e-04, -1.32707180e-04],\n",
       "        [-3.93878698e-04,  2.09012505e-04, -4.16698458e-04, ...,\n",
       "          6.92115107e-04,  6.19169034e-04, -4.38627903e-04],\n",
       "        [-3.66414664e-04,  2.73932674e-04, -7.36405957e-04, ...,\n",
       "          2.89068732e-04,  1.16875733e-03, -7.53035303e-04],\n",
       "        ...,\n",
       "        [-8.30025470e-04, -2.30339123e-03,  2.90704775e-03, ...,\n",
       "          7.12369860e-04, -1.50452706e-03,  1.10359385e-03],\n",
       "        [-1.41747575e-03, -2.22029909e-03,  3.44137684e-03, ...,\n",
       "          7.40123505e-04, -1.59859087e-03,  1.06790289e-03],\n",
       "        [-1.96510879e-03, -2.11719703e-03,  3.92261567e-03, ...,\n",
       "          7.59857765e-04, -1.65833789e-03,  1.00644061e-03]],\n",
       "\n",
       "       [[-4.64363227e-04,  1.61830278e-04, -3.14673467e-04, ...,\n",
       "          4.21569595e-04,  2.52897298e-04, -1.32707180e-04],\n",
       "        [-6.10928924e-04,  4.63360222e-04, -4.54876805e-04, ...,\n",
       "          7.95401051e-04,  2.79355358e-04, -2.35474319e-04],\n",
       "        [-7.56307680e-04,  5.26137359e-04, -4.64349228e-04, ...,\n",
       "          1.08827243e-03, -1.67302453e-04, -3.01665510e-04],\n",
       "        ...,\n",
       "        [-1.46137562e-03, -1.50539319e-03,  2.87412456e-03, ...,\n",
       "         -5.38309629e-04, -6.07986876e-04,  2.09427948e-04],\n",
       "        [-1.93088746e-03, -1.49951479e-03,  3.45050357e-03, ...,\n",
       "         -3.42199462e-04, -9.30223847e-04,  3.14084435e-04],\n",
       "        [-2.39150133e-03, -1.46561477e-03,  3.98039352e-03, ...,\n",
       "         -1.70154613e-04, -1.18297001e-03,  3.86447646e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for source_sample, target_sample in dataset.take(1):\n",
    "    break\n",
    "\n",
    "model(source_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45b46960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bab2dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 38s 384ms/step - loss: 3.6467\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 36s 385ms/step - loss: 2.8487\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 36s 386ms/step - loss: 2.7719\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 36s 386ms/step - loss: 2.7011\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 36s 385ms/step - loss: 2.6312\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 36s 387ms/step - loss: 2.5744\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 36s 387ms/step - loss: 2.5285\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 36s 387ms/step - loss: 2.4869\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 25s 268ms/step - loss: 2.4491\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 2.4110\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 2.3760\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 2.3407\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 2.3079\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 2.2779\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 2.2462\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 2.2149\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 2.1856\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 2.1558\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 2.1284\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 2.1002\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 2.0706\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 2.0405\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 2.0110\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 1.9813\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 1.9523\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 1.9234\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 1.8932\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 1.8628\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 1.8346\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 1.8065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5fdc3ce4f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaebb366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    while True:\n",
    "        predict = model(test_tensor)\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token:\n",
    "            break\n",
    "        if test_tensor.shape[1] >= max_len: \n",
    "            break\n",
    "\n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27228314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i ll tell you , sir , i ll tell you . <end> '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate_text(model, tokenizer, init_sentence=\"<start> he\")\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> I\")\n",
    "# generate_text(model, tokenizer, init_sentence=\"<start> she\")\n",
    "# generate_text(model, tokenizer, init_sentence=\"<start>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515306e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
